{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits.\n",
    "\n",
    "The particular dataset we will use is the MNIST (Modified National Institute of Standards and Technology)\n",
    "The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a machine to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/scratch1/hendriks/miniconda3/envs/odl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "In these exercies we'll use the software library [tensorflow](https://www.tensorflow.org/), which is a state of the art library for numerical computations in python developed by Google. The package supports computation on both the CPU and GPU, and also supports automatic differentiation (more on that later).\n",
    "\n",
    "Writing code in tensorflow is quite similar to classical python, except for one thing: tensorflow uses *deffered execution*, this means that instead of executing the code in one step (called eager execution), the computational graph is first defined, and it is then fed data upon which the computation is performed. Users from compiled languages will recongnize this execution method.\n",
    "\n",
    "The execution flow typically goes like this:\n",
    "\n",
    "* Define placeholders for the input\n",
    "* Define what computations should be performed\n",
    "* Feed the placeholders data\n",
    "\n",
    "This computational model has several upsides that are sadly not evident in simple samples like these, including parallelization, packaging and optimization.\n",
    "\n",
    "In addition to this, tensorflow uses scopes (e.g. [`name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope)) to organize the code. This can interact very nicely with external tools like [tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard), but is also used as comments in the code and to give better error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Set the random seed to enable reproducible code\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and utilities\n",
    "\n",
    "We now need to get the data we will use, which in this case is the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of digits 70000 hand-written digits, of which 60000 are used for training and 10000 for testing.\n",
    "\n",
    "In addition to this, we create a utility `evaluate(...)` that we will use to evaluate how good the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdac83f8940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[0].reshape(28,-1),cmap='Greys_r');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the 10000 mnist test points and define a method that we'll use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = mnist.test.next_batch(10000)\n",
    "test_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "test_labels = batch[1]\n",
    "\n",
    "def evaluate(result_tensor, data_placeholder):\n",
    "    \"\"\"Evaluate a reconstruction method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_tensor : `tf.Tensor`, shape (None,)\n",
    "        The tensorflow tensor containing the result of the classification.\n",
    "    data_placeholder : `tf.Tensor`, shape (None, 28, 28, 1) or (None, 784)\n",
    "        The tensorflow tensor containing the input to the classification operator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : float\n",
    "        Mean squared error of the reconstruction.\n",
    "    \"\"\"\n",
    "    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n",
    "    result = result_tensor.eval(\n",
    "        feed_dict={data_placeholder: feed_images})\n",
    "\n",
    "    return np.mean(result == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We start with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), perhaps the most well known and widely applied classification method.\n",
    "\n",
    "The first problem we need to solve is that the values we try to regress against are discrete (e.g. [0, 1, 2, ..., 9]) which does not work very well with continuous optimization. To solve this we convert the values to a one-hot encoding, embedding the values into $\\mathbb{R}^{10}$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toh = tf.one_hot([0, 1, 2], depth=3)\n",
    "toh.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as a probabilistic encoding, i.e. we can estimate that a number is 10% 1 and 90% 2. For our training data, we have 100% certanity for each digit. \n",
    "\n",
    "The estimator used for logistic regression is\n",
    "\n",
    "$$\n",
    "p_x(\\text{label=$i$}) = \\frac{\\exp(\\langle w_i, x \\rangle + b_i)}{\\sum_{j=0}^9 \\exp(\\langle w_j, x \\rangle + b_j)}\n",
    "$$\n",
    "\n",
    "Here, $p_x(\\text{label=$i$})$ is the probability of an image $x$ belonging to a category $i$, $w_i \\in \\mathbb{R}^{28 \\times 28}$ and $b_i \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is a comparison between the probability distribution $p_x$ and the deterministic probability distribution $q_x$.\n",
    "We use the *cross entropy* for this. The loss function for each image is:\n",
    "\\\\[\n",
    "-\\sum_{i=0}^9 q_x(\\text{label=$i$}) \\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is the mean value of the cross entropy (implicitly assuming that the images are uniformly distributed):\n",
    "\\\\[\n",
    "L(p) := -\\frac{1}{N}\\sum_{x=1}^N\\sum_{i=0}^9 q_x(\\text{label=$i$})\\ln(p_x(\\text{label=$i$}))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an elementary implementation in `TensorFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('elementary_network'):\n",
    "    # Create a placeholder for our input data (no computation is done here)\n",
    "    inp = tf.placeholder(shape=(None, 784), dtype=tf.float32, name=\"input\")\n",
    "    \n",
    "    # Create the parameters (weight, bias) of the model\n",
    "    weights = tf.Variable(tf.random_normal((784, 10)), name=\"weights\")\n",
    "    bias = tf.Variable(tf.zeros((10)), name=\"bias\")\n",
    "    \n",
    "    # Compute the probabilities (this is all lazy, no computations are actually performed)\n",
    "    lin = tf.matmul(inp, weights) + bias\n",
    "    elin = tf.exp(lin)\n",
    "    Z = tf.reduce_sum(elin, axis=1, keep_dims=True)\n",
    "    prob = elin / Z\n",
    "    log_prob = tf.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function which measures how good our parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_loss\"):\n",
    "    labels = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    determ = tf.one_hot(labels, depth=10)\n",
    "    loss = -tf.reduce_mean(determ*log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "We'll train the network using gradient descent, i.e.\n",
    "\n",
    "$$w_i \\leftarrow w_i - \\omega \\nabla_{w_i} L(w, b)$$\n",
    "\n",
    "where $\\omega$ is the *learning rate*, or step size.\n",
    "\n",
    "Note that in machine learning, we typically use *stochastic* gradient descent (SGD). In these methods we don't use all of the data to compute the gradient, only a small subset called a mini-batch. Here we use 128 images in each training step.\n",
    "\n",
    "Further, while for this case computing the gradient would be quite simple, once we move to harder and mroe complicated models doing so would be basically impossible to do by hand. To work around this, all major deep learning frameworks implement [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). This may sound fancy, but automatic differentiation is simply the chain rule for the derivative. Tensorflow implements it using the `tf.gradients` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_training\"):\n",
    "    learning_rate = .1\n",
    "\n",
    "    variables = [weights, bias]\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    update_ops = [var.assign(var - learning_rate*grad) \n",
    "                  for var, grad in zip(variables, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the code above was lazy, nothing has actually happened. Before we start we need to initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network by feeding data from the training set and occationally evalute the performance on our test set, this is the first point we actually start doing computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2%, 45.0%, 60.6%, 68.5%, 72.4%, 75.1%, 77.2%, 78.7%, 80.0%, 81.0%, 81.8%, 82.4%, 82.9%, 83.4%, 83.7%, 84.1%, 84.5%, 84.8%, 85.0%, 85.2%, 85.4%, 85.6%, 85.8%, 85.9%, 86.1%, 86.3%, 86.4%, 86.6%, 86.7%, 86.8%, 86.9%, 87.1%, 87.1%, 87.2%, 87.4%, 87.5%, 87.5%, 87.6%, 87.7%, 87.8%, 87.9%, 88.0%, 88.0%, 88.0%, 88.1%, 88.1%, 88.3%, 88.2%, 88.3%, 88.3%, 88.4%, 88.4%, 88.4%, 88.4%, 88.5%, 88.5%, 88.6%, 88.6%, 88.6%, 88.6%, 88.7%, 88.7%, 88.9%, 88.9%, 88.9%, 88.8%, 88.9%, 89.0%, 88.9%, 89.0%, 89.0%, 89.1%, 89.1%, 89.1%, 89.2%, 89.1%, 89.2%, 89.2%, 89.2%, 89.3%, 89.3%, 89.3%, 89.3%, 89.4%, 89.4%, 89.4%, 89.4%, 89.5%, 89.5%, 89.4%, 89.4%, 89.5%, 89.5%, 89.6%, 89.6%, 89.6%, 89.5%, 89.7%, 89.5%, 89.6%, "
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(update_ops, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(log_prob, axis=1), inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow libraries\n",
    "\n",
    "While the above code solves our problem, it involved several small and perhaps obscure steps. Once we start moving to more complicated neural networks the code would become very repetetive.\n",
    "\n",
    "Since all of the steps are standardized, we can (and should) instead use built in tensorflow functions, this example does that, and all following examples will do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "The \"network\" can be computed using the [`tf.contrib.layers.fully_connected`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) function which computes\n",
    "\n",
    "$$\\rho(Wx + b)$$\n",
    "\n",
    "where $\\rho$ is the activation function, $W$ is a matrix called the weights and $b$ is a real vector called the bias. Note that here we never explicitly construct these, they are hidden inside tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    logits = tf.contrib.layers.fully_connected(inp, \n",
    "                                               num_outputs=10,      # We only need to specify the number of outputs\n",
    "                                               activation_fn=None)  # Dont use any activation (include in loss instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimization\n",
    "\n",
    "The loss function defined above should be done using the [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) function, which not only is easier to use, it is also more numerically stable.\n",
    "\n",
    "In addition to this, we don't really have to write our own optimizer, and there are in fact several very good optimizers built into tensorflow. In this example we'll use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) which is a very popular and efficient gradient-based optimizer. All of the details regarding weights etc are hidden inside tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "Training the network looks about the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2%, 91.5%, 92.2%, 92.4%, 92.5%, 92.6%, 92.7%, 92.7%, 92.7%, 92.8%, "
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "The first \"deep\" neural networks were [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron), in these we have a function of the following form\n",
    "\n",
    "$$\n",
    "\\rho(W_3\\rho(W_2\\rho(W_1 x + b_1) + b_2) + b_3)\n",
    "$$\n",
    "\n",
    "Where $W_i$ are matrices and $b_i$ vectors. Note that the logistic regression can be cast into this form (how?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7%, 96.5%, 97.1%, 97.5%, 97.5%, 97.6%, 97.6%, 97.9%, 97.9%, 97.9%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.fully_connected(inp, num_outputs=128)  # the default activation function is ReLU\n",
    "    x = tf.contrib.layers.fully_connected(x, num_outputs=32)\n",
    "    logits = tf.contrib.layers.fully_connected(x, \n",
    "                                               num_outputs=10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Convolutional neural networks are a corner-stone of the deep learning revolution. Here instead of using traditionall fully-connected layers which connect each point with all other points, we use spatial convolutions instead. By doing this, we get a translation invariant operator that acts locally. \n",
    "\n",
    "A convolution operator $C : \\mathcal{X}^n \\to \\mathcal{X}^m$, where $\\mathcal{X}$ is some set of images and $n$ is called the number of channels, is defined by\n",
    "$$\n",
    "[C(x)]_i = \\sum_j (w_{ij} \\ast x_j + b_j)\n",
    "$$\n",
    "where the *weight* $w_{ij}$ is a convolution kernel and $b_j \\in \\mathbb{R}$ is the bias.\n",
    "\n",
    "In applications, there number of duplicate entries $n, m$ is typically $\\in [32, 512]$ and the convolution kernel is given by a small $e.g. 3 \\times 3$ stencil. We'll use the built in function [`tf.contrib.layers.conv2d`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d) in order to perform the convolutions efficiently.\n",
    "\n",
    "In order to get non-local behaviour we stack several of these on top of each other in the same manner as a MLP.\n",
    "\n",
    "The following code is a very simplified convolutional neural network for digit classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.2%, 98.2%, 98.7%, 98.6%, 98.7%, 98.8%, 98.8%, 98.9%, 98.9%, 98.6%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('convolutional_network'):\n",
    "    # Reshape the input from [?, 784] to [?, 28, 28, 1] where 1 is the number of channels (e.g. n above)\n",
    "    images = tf.reshape(inp, [-1, 28, 28, 1])\n",
    "    \n",
    "    x = tf.contrib.layers.conv2d(images, \n",
    "                                 num_outputs=32, # Number of \"channels\", e.g. duplicates of the image\n",
    "                                 kernel_size=3,  # size of the convolution kernel\n",
    "                                 stride=2)       # Use strides (jumps) to decrease the image size in each step\n",
    "    # x.shape = [?, 14, 14, 32]\n",
    "    x = tf.contrib.layers.conv2d(x, num_outputs=32, kernel_size=3, stride=2)\n",
    "    # x.shape = [?, 7, 7, 32]\n",
    "    x = tf.contrib.layers.flatten(x)    \n",
    "    # x.shape = [?, 1568]\n",
    "    \n",
    "    # It is typically a good idea to finish with fully connected layers \n",
    "    # in order to encode the non-translation invariant information\n",
    "    x = tf.contrib.layers.fully_connected(x, 128)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"{:.1f}%, \".format(evaluate(tf.argmax(logits, axis=1), inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005,\n",
       " 98.870000000000005]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batch_loss = []\n",
    "for i in range(10):\n",
    "    inp_, labels_ = mnist.validation.next_batch(128)\n",
    "    session.run(logits, feed_dict={labels: labels_, inp:inp_})\n",
    "    val_batch_loss.append(evaluate(tf.argmax(logits, axis=1), inp) * 100)\n",
    "    \n",
    "val_batch_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "From this notebook there are some take-aways\n",
    "\n",
    "* A classification problem involves taking an image and assigning it to a class (e.g. 4)\n",
    "* Several methods from machine learning can be used for classificication\n",
    "* Software libraries like tensorflow are great\n",
    "* Deeper neural networks work very well\n",
    "* Convolutions allow us to encode spatial information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
